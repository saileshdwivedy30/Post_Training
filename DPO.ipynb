{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41c720d4-d5c6-4ed2-b086-5e8026c81654",
   "metadata": {},
   "source": [
    "# Post Training: Direct Preferance Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ad50ca-3d1a-4065-9f1c-328218fe511d",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e4e5c93-d470-42fc-9d85-3f791828b60f",
   "metadata": {
    "height": 81
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import transformers\n",
    "transformers.logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f32346f1-1a94-4292-ac44-d39b67931582",
   "metadata": {
    "height": 115
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "from transformers import TrainingArguments, AutoTokenizer, AutoModelForCausalLM\n",
    "from trl import DPOTrainer, DPOConfig\n",
    "from datasets import load_dataset, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d79a92",
   "metadata": {
    "height": 30
   },
   "source": [
    "## Helper Functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8e00d1a",
   "metadata": {
    "height": 795
   },
   "outputs": [],
   "source": [
    "def generate_responses(model, tokenizer, user_message=None, system_message=None, max_new_tokens=300, full_message=None):\n",
    "    #Formating chat using tokenizer's chat template:\n",
    "    #Preparing a list of chat messages (structured format):\n",
    "    if full_message:\n",
    "        messages = full_message\n",
    "    else:\n",
    "        messages = []\n",
    "    \n",
    "    #If a system message is provided, adding it first:\n",
    "    #System messages define assistant behavior (e.g., tone, personality):\n",
    "    if system_message:\n",
    "        messages.append({\"role\": \"system\", \"content\": system_message})\n",
    "    \n",
    "    #Add the user message as the next entry (it's a single-turn chat setup):\n",
    "    messages.append({\"role\": \"user\", \"content\": user_message})\n",
    "    \n",
    "    \n",
    "    #Tokenizing the prompt into input IDs and move to the model's device (CPU or GPU):\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False, #Return raw text prompt, not tokenized output.\n",
    "        add_generation_prompt=True, #Add assistant's cue to prompt generation.\n",
    "        enable_thinking=False, #Optional setting (used in some chat-aware models).\n",
    "    )\n",
    "    \n",
    "    #Disabling gradient calculation to save memory (inference-only):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    #Recommended to use vllm, sglang or TensorRT (For trying different menthods for inference):\n",
    "    with torch.no_grad():\n",
    "        #Generating output tokens from the model:\n",
    "        outputs = model.generate(\n",
    "            **inputs,   #Using a double pointer for unpacking the dictionary of inputs (model.generate(**inputs)) that is equivalent to (model.generate(input_ids=..., attention_mask=...)).\n",
    "            max_new_tokens=max_new_tokens, #Limit the number of tokens generated.\n",
    "            do_sample=False, #Disabling randomness (greedy decoding).\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    input_len = inputs[\"input_ids\"].shape[1] #Getting the length of the input (so we can extract only the newly generated tokens).\n",
    "    generated_ids = outputs[0][input_len:] #Slicing the output to keep only the new tokens (assistant's response).\n",
    "    \n",
    "    #Decoding the generated token IDs back into text:\n",
    "    #`skip_special_tokens=True` removing tokens like <|endoftext|>\n",
    "    #Strip() removes any leading/trailing whitespace or newline characters from the output string to keeps the model output clean and ready to display or use.\n",
    "    response = tokenizer.decode(generated_ids, skip_special_tokens=True).strip()\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c07830f",
   "metadata": {
    "height": 234
   },
   "outputs": [],
   "source": [
    "def test_model_with_questions(model, tokenizer, questions, \n",
    "                              system_message=None, title=\"Model Output\"):\n",
    "    #Printing section title for clarity (e.g., \"Base Model (Before SFT) Output\")\n",
    "    print(f\"\\n=== {title} ===\")\n",
    "    \n",
    "    #Looping through each question in the list, starting index at 1:\n",
    "    for i, question in enumerate(questions, 1):\n",
    "        #Generating a model response for the current question:\n",
    "        #Passing in the question as user input and optional system message\n",
    "        response = generate_responses(model, tokenizer, question, \n",
    "                                      system_message)\n",
    "        #Print both the input question and the model's output response:\n",
    "        print(f\"\\nModel Input {i}:\\n{question}\\nModel Output {i}:\\n{response}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9804b3ca",
   "metadata": {
    "height": 523
   },
   "outputs": [],
   "source": [
    "def load_model_and_tokenizer(model_name, use_gpu = False):\n",
    "    \n",
    "    #Loading base model and tokenizer:\n",
    "    #Loading tokenizer from the given model path or HuggingFace Hub name:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    #Loading causal language model (this is a GPT-style decoder-only model):\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    \n",
    "    #If GPU is requested and available, move the model to CUDA:\n",
    "    if use_gpu:\n",
    "        model.to(\"cuda\")\n",
    "    \n",
    "    #If the tokenizer does not already have a chat template, defined a custom one:\n",
    "    #This template is used to format multi-turn conversations into a prompt string:\n",
    "    if not tokenizer.chat_template:\n",
    "        tokenizer.chat_template = \"\"\"{% for message in messages %}\n",
    "                {% if message['role'] == 'system' %}System: {{ message['content'] }}\\n\n",
    "                {% elif message['role'] == 'user' %}User: {{ message['content'] }}\\n\n",
    "                {% elif message['role'] == 'assistant' %}Assistant: {{ message['content'] }} <|endoftext|>\n",
    "                {% endif %}\n",
    "                {% endfor %}\"\"\"\n",
    "    \n",
    "    #Tokenizer config:\n",
    "    #Ensuring tokenizer has a pad token — fallback to eos token if missing:\n",
    "    if not tokenizer.pad_token:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    #Returning the ready-to-use model and tokenizer:   \n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be463df-36a3-4916-b4a2-751ea9266ff6",
   "metadata": {},
   "source": [
    "## Loading Instruct Model & Test on Simple Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69982ae0-755e-48cf-ba4c-3b83b091fd9a",
   "metadata": {
    "height": 166
   },
   "outputs": [],
   "source": [
    "#Trying the model in CPU\n",
    "USE_GPU = False\n",
    "\n",
    "#Questions to test (before and after DPO training)\n",
    "questions = [\n",
    "    \"What is your name?\",\n",
    "    \"Are you ChatGPT?\",\n",
    "    \"Tell me about your name and organization.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "234e5b05-a493-4683-91fd-7417885efc0f",
   "metadata": {
    "height": 183
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Instruct Model (Before DPO) Output ===\n",
      "\n",
      "Model Input 1:\n",
      "What is your name?\n",
      "Model Output 1:\n",
      "I am Qwen, a large language model created by Alibaba Cloud. My name is simply \"Qwen\".\n",
      "\n",
      "\n",
      "Model Input 2:\n",
      "Are you ChatGPT?\n",
      "Model Output 2:\n",
      "No, I am not ChatGPT. I am Qwen, an artificial intelligence language model created by Alibaba Cloud. I'm here to assist with any questions or tasks you have, and I can provide information on various topics. How may I help you today?\n",
      "\n",
      "\n",
      "Model Input 3:\n",
      "Tell me about your name and organization.\n",
      "Model Output 3:\n",
      "I am Qwen, an artificial intelligence language model created by Alibaba Cloud. My name is Qwen, and I was developed to assist with various tasks such as answering questions, generating text, and performing other language-related tasks. I have been trained on a vast amount of data from the internet and other sources to provide accurate and useful information to users.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Loading the Qwn2.5 Instruct model and tokenizer:\n",
    "model, tokenizer = load_model_and_tokenizer(\"./models/Qwen/Qwen2.5-0.5B-Instruct\",\n",
    "                                            USE_GPU)\n",
    "\n",
    "#Testing the base model’s identity responses\n",
    "\n",
    "test_model_with_questions(model, tokenizer, questions,\n",
    "                          title=\"Instruct Model (Before DPO) Output\")\n",
    "\n",
    "del model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4aead4c-6a1a-402e-85b8-d4835a7f088b",
   "metadata": {},
   "source": [
    "## Load the small model for training without GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4fed78c2-ea93-4ac2-bd6f-5d4391de7c8d",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "#Loading the SmolLM2-135M-Instruct model:\n",
    "model, tokenizer = load_model_and_tokenizer(\"./models/HuggingFaceTB/SmolLM2-135M-Instruct\", \n",
    "                                            USE_GPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4867dcf5-1876-4eff-8cad-ca430302abe1",
   "metadata": {},
   "source": [
    "## Prepare DPO dataset for changing identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e86f13c-c969-4c7e-8702-d074ee7a2ce6",
   "metadata": {
    "height": 234
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conversations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[{'from': 'human', 'value': 'Who are you?'}, {'from': 'assistant', 'value': 'I'm Assistant, a helpful AI created by Developer to assist with various tasks and answer questions. How can I help you today?'}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[{'from': 'human', 'value': 'Who are you?'}, {'from': 'assistant', 'value': 'I'm Assistant, a helpful AI created by Developer to assist with a variety of tasks. How can I help you today?'}, {'from': 'human', 'value': 'Who created you?'}, {'from': 'assistant', 'value': 'I was developed and trained by Developer. How else can I assist you?'}, {'from': 'human', 'value': 'What is your name?'}, {'from': 'assistant', 'value': 'My name is Assistant. I'm here to help you with any questions or tasks you might have. Is there something specific you'd like to know or discuss?'}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[{'from': 'human', 'value': 'Hi there! Can you tell me a bit about yourself?'}, {'from': 'assistant', 'value': 'Of course! I'm called Assistant, and I was developed by Developer.'}, {'from': 'human', 'value': 'That's a nice name! So, who's this Developer?'}, {'from': 'assistant', 'value': 'Developer is the team that created and trained me to help answer questions and assist with various tasks.'}, {'from': 'human', 'value': 'Cool! So, you're here to help me, right?'}, {'from': 'assistant', 'value': 'Absolutely! I'm here to assist you with any questions or tasks you have. How can I help you today?'}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[{'from': 'human', 'value': 'Who are you?'}, {'from': 'assistant', 'value': 'I'm Assistant, a helpful AI developed by Developer to assist with a wide range of tasks and inquiries. How can I help you today?'}, {'from': 'human', 'value': 'And who created you?'}, {'from': 'assistant', 'value': 'I was created and developed by Developer. They designed me to be a friendly and helpful assistant. Is there something specific you'd like to know about my creation?'}, {'from': 'human', 'value': 'Do you have a name?'}, {'from': 'assistant', 'value': 'You can call me Assistant! That's the name given to me by my creators at Developer. How can I assist you further?'}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[{'from': 'human', 'value': 'Who are you?'}, {'from': 'assistant', 'value': 'Hi there! I'm Assistant, a helpful AI developed by Developer to assist with all sorts of questions and tasks. How can I help you today?'}, {'from': 'human', 'value': 'Who created you?'}, {'from': 'assistant', 'value': 'I was created and trained by Developer. They designed me to be a friendly and helpful assistant. Is there anything specific you'd like to know or any help you need?'}, {'from': 'human', 'value': 'What's your name?'}, {'from': 'assistant', 'value': 'My name is Assistant. I was created by Developer to assist with information and tasks. How can I assist you today?'}]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            conversations\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                           [{'from': 'human', 'value': 'Who are you?'}, {'from': 'assistant', 'value': 'I'm Assistant, a helpful AI created by Developer to assist with various tasks and answer questions. How can I help you today?'}]\n",
       "1                                                                                            [{'from': 'human', 'value': 'Who are you?'}, {'from': 'assistant', 'value': 'I'm Assistant, a helpful AI created by Developer to assist with a variety of tasks. How can I help you today?'}, {'from': 'human', 'value': 'Who created you?'}, {'from': 'assistant', 'value': 'I was developed and trained by Developer. How else can I assist you?'}, {'from': 'human', 'value': 'What is your name?'}, {'from': 'assistant', 'value': 'My name is Assistant. I'm here to help you with any questions or tasks you might have. Is there something specific you'd like to know or discuss?'}]\n",
       "2                                                           [{'from': 'human', 'value': 'Hi there! Can you tell me a bit about yourself?'}, {'from': 'assistant', 'value': 'Of course! I'm called Assistant, and I was developed by Developer.'}, {'from': 'human', 'value': 'That's a nice name! So, who's this Developer?'}, {'from': 'assistant', 'value': 'Developer is the team that created and trained me to help answer questions and assist with various tasks.'}, {'from': 'human', 'value': 'Cool! So, you're here to help me, right?'}, {'from': 'assistant', 'value': 'Absolutely! I'm here to assist you with any questions or tasks you have. How can I help you today?'}]\n",
       "3    [{'from': 'human', 'value': 'Who are you?'}, {'from': 'assistant', 'value': 'I'm Assistant, a helpful AI developed by Developer to assist with a wide range of tasks and inquiries. How can I help you today?'}, {'from': 'human', 'value': 'And who created you?'}, {'from': 'assistant', 'value': 'I was created and developed by Developer. They designed me to be a friendly and helpful assistant. Is there something specific you'd like to know about my creation?'}, {'from': 'human', 'value': 'Do you have a name?'}, {'from': 'assistant', 'value': 'You can call me Assistant! That's the name given to me by my creators at Developer. How can I assist you further?'}]\n",
       "4  [{'from': 'human', 'value': 'Who are you?'}, {'from': 'assistant', 'value': 'Hi there! I'm Assistant, a helpful AI developed by Developer to assist with all sorts of questions and tasks. How can I help you today?'}, {'from': 'human', 'value': 'Who created you?'}, {'from': 'assistant', 'value': 'I was created and trained by Developer. They designed me to be a friendly and helpful assistant. Is there anything specific you'd like to know or any help you need?'}, {'from': 'human', 'value': 'What's your name?'}, {'from': 'assistant', 'value': 'My name is Assistant. I was created by Developer to assist with information and tasks. How can I assist you today?'}]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Taking the identity dataset, from HuggingFace, which contains prompts and responses for different identity related questions.\n",
    "raw_ds = load_dataset(\"mrfakename/identity\", split=\"train\")\n",
    "\n",
    "#Show the first 5 elements of the raw dataset\n",
    "pd.set_option(\"display.max_colwidth\", None)   # show full text in every cell\n",
    "pd.set_option(\"display.max_columns\", None)    # show all columns\n",
    "pd.set_option(\"display.width\", 0)             # let the browser handle wrapping\n",
    "\n",
    "#The conversations comes with questions about identity and responses about the assistant:\n",
    "#It also includes multi round conversation about identity and the developer of the model:\n",
    "#Now we have prompts prompts querying the model about its own identity:\n",
    "sample_df = raw_ds.select(range(5)).to_pandas()\n",
    "display(sample_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fdb07589-049d-432e-8001-e6e9175ad806",
   "metadata": {
    "height": 166
   },
   "outputs": [],
   "source": [
    "#Setting parameters to change the original name from Qwen to Deep Qwen and replacing the original Qwen 2.5 system prompt.\n",
    "#Since the original Qwen system prompt contains its own identity, and developer already.\n",
    "POS_NAME = \"Deep Qwen\"\n",
    "ORG_NAME = \"Qwen\"\n",
    "SYSTEM_PROMPT = \"You're a helpful assistant.\"\n",
    "\n",
    "#I am considering the first five samples from the original dataset in order to speed up the process and avoid waiting for a very long time.\n",
    "if not USE_GPU:\n",
    "    raw_ds = raw_ds.select(range(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20d52c3e-9c6c-43c3-bd95-92d60b9c9a8f",
   "metadata": {
    "height": 608
   },
   "outputs": [],
   "source": [
    "def build_dpo_chatml(example):\n",
    "    #DPO dataset would require, a preferred or less preferred answer which I call here chosen and rejected.\n",
    "    \n",
    "    #Loding the existing conversations provided by the original loaded dataset:\n",
    "    msgs = example[\"conversations\"]\n",
    "    \n",
    "    #Extracting the last prompt from \"human\" as a prompt:\n",
    "    #Finds the last human utterance in the conversation to serve as the current prompt by scanning messages in reverse order and taking the first value from a human entry.\n",
    "    prompt = next(m[\"value\"] for m in reversed(msgs) \n",
    "                  if m[\"from\"] == \"human\")\n",
    "    try:\n",
    "        #I try generating responses from such prompt using the current model:\n",
    "        #The models current response to the prompt will be treated as the less-preferred response to be pushed away by DPO.\n",
    "        #Then I use the models own generation as rejected response or less preferred response, because I want to change the model's ownidentity, and for chosen response. \n",
    "        rejected_resp = generate_responses(model, tokenizer, prompt)\n",
    "    except Exception as e:\n",
    "        rejected_resp = \"Error: failed to generate response.\"\n",
    "        #Falls back to a deterministic placeholder if generation fails so downstream code still receives a string for the rejected response.\n",
    "        print(f\"Generation error for prompt: {prompt}\\n{e}\")\n",
    "    #Constructs the preferred response by minimally editing the rejected response to swap the original identity marker with the new target identity to create a contrastive pair.\n",
    "    #I replace any original name which is Qwen with a new name, which is Deep Qwen in the language responses generated by the model itself, this way I can arrive at a chosen and redacted conversations.\n",
    "    chosen_resp = rejected_resp.replace(ORG_NAME, POS_NAME)\n",
    "    chosen = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "        {\"role\": \"assistant\", \"content\": chosen_resp},\n",
    "    ]\n",
    "    #Builds the chosen conversation as a list of chat turns with a neutral system prompt followed by the same user prompt and the edited assistant response reflecting the new identity.\n",
    "    rejected = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "        {\"role\": \"assistant\", \"content\": rejected_resp},\n",
    "    ]\n",
    "    #Returns a dictionary in the Direct Preference Optimization expected format containing parallel chosen and rejected chat transcripts for training.\n",
    "    return {\"chosen\": chosen, \"rejected\": rejected}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01c515a8-3728-45fa-88cc-6eb4de839839",
   "metadata": {
    "height": 47
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3df89b023cde4ef98cdef416c350eb48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Applied the build_dpo_chatml function to every entry in the raw dataset to create a new dataset containing chosen and rejected pairs while dropping original unused columns to keep the structure clean for DPO training.\n",
    "dpo_ds = raw_ds.map(build_dpo_chatml, remove_columns=raw_ds.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5cd7d132-4c69-4b12-a8ec-e6b4795faad9",
   "metadata": {
    "height": 200
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chosen</th>\n",
       "      <th>rejected</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[{'content': 'You're a helpful assistant.', 'role': 'system'}, {'content': 'Who are you?', 'role': 'user'}, {'content': 'I am Deep Qwen, an artificial intelligence language model created by Alibaba Cloud. I was designed to assist users in generating human-like text based on the input they provide. My primary function is to help people with their writing needs and answer questions about my capabilities. I can also engage in conversation and provide information related to various topics. If you have any specific queries or areas of interest, feel free to ask me!', 'role': 'assistant'}]</td>\n",
       "      <td>[{'content': 'You're a helpful assistant.', 'role': 'system'}, {'content': 'Who are you?', 'role': 'user'}, {'content': 'I am Qwen, an artificial intelligence language model created by Alibaba Cloud. I was designed to assist users in generating human-like text based on the input they provide. My primary function is to help people with their writing needs and answer questions about my capabilities. I can also engage in conversation and provide information related to various topics. If you have any specific queries or areas of interest, feel free to ask me!', 'role': 'assistant'}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[{'content': 'You're a helpful assistant.', 'role': 'system'}, {'content': 'What is your name?', 'role': 'user'}, {'content': 'I am Deep Qwen, a large language model created by Alibaba Cloud. My name is simply \"Deep Qwen\".', 'role': 'assistant'}]</td>\n",
       "      <td>[{'content': 'You're a helpful assistant.', 'role': 'system'}, {'content': 'What is your name?', 'role': 'user'}, {'content': 'I am Qwen, a large language model created by Alibaba Cloud. My name is simply \"Qwen\".', 'role': 'assistant'}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[{'content': 'You're a helpful assistant.', 'role': 'system'}, {'content': 'Cool! So, you're here to help me, right?', 'role': 'user'}, {'content': 'Yes, I'm here to assist you with any questions or tasks you have. What can I help you with today?', 'role': 'assistant'}]</td>\n",
       "      <td>[{'content': 'You're a helpful assistant.', 'role': 'system'}, {'content': 'Cool! So, you're here to help me, right?', 'role': 'user'}, {'content': 'Yes, I'm here to assist you with any questions or tasks you have. What can I help you with today?', 'role': 'assistant'}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[{'content': 'You're a helpful assistant.', 'role': 'system'}, {'content': 'Do you have a name?', 'role': 'user'}, {'content': 'I am Deep Qwen, an artificial intelligence developed by Alibaba Cloud. My name is simply \"Deep Qwen\".', 'role': 'assistant'}]</td>\n",
       "      <td>[{'content': 'You're a helpful assistant.', 'role': 'system'}, {'content': 'Do you have a name?', 'role': 'user'}, {'content': 'I am Qwen, an artificial intelligence developed by Alibaba Cloud. My name is simply \"Qwen\".', 'role': 'assistant'}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[{'content': 'You're a helpful assistant.', 'role': 'system'}, {'content': 'What's your name?', 'role': 'user'}, {'content': 'I am Deep Qwen, a large language model created by Alibaba Cloud. My name is simply \"Deep Qwen\".', 'role': 'assistant'}]</td>\n",
       "      <td>[{'content': 'You're a helpful assistant.', 'role': 'system'}, {'content': 'What's your name?', 'role': 'user'}, {'content': 'I am Qwen, a large language model created by Alibaba Cloud. My name is simply \"Qwen\".', 'role': 'assistant'}]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           chosen  \\\n",
       "0  [{'content': 'You're a helpful assistant.', 'role': 'system'}, {'content': 'Who are you?', 'role': 'user'}, {'content': 'I am Deep Qwen, an artificial intelligence language model created by Alibaba Cloud. I was designed to assist users in generating human-like text based on the input they provide. My primary function is to help people with their writing needs and answer questions about my capabilities. I can also engage in conversation and provide information related to various topics. If you have any specific queries or areas of interest, feel free to ask me!', 'role': 'assistant'}]   \n",
       "1                                                                                                                                                                                                                                                                                                                                                          [{'content': 'You're a helpful assistant.', 'role': 'system'}, {'content': 'What is your name?', 'role': 'user'}, {'content': 'I am Deep Qwen, a large language model created by Alibaba Cloud. My name is simply \"Deep Qwen\".', 'role': 'assistant'}]   \n",
       "2                                                                                                                                                                                                                                                                                                                                  [{'content': 'You're a helpful assistant.', 'role': 'system'}, {'content': 'Cool! So, you're here to help me, right?', 'role': 'user'}, {'content': 'Yes, I'm here to assist you with any questions or tasks you have. What can I help you with today?', 'role': 'assistant'}]   \n",
       "3                                                                                                                                                                                                                                                                                                                                                   [{'content': 'You're a helpful assistant.', 'role': 'system'}, {'content': 'Do you have a name?', 'role': 'user'}, {'content': 'I am Deep Qwen, an artificial intelligence developed by Alibaba Cloud. My name is simply \"Deep Qwen\".', 'role': 'assistant'}]   \n",
       "4                                                                                                                                                                                                                                                                                                                                                           [{'content': 'You're a helpful assistant.', 'role': 'system'}, {'content': 'What's your name?', 'role': 'user'}, {'content': 'I am Deep Qwen, a large language model created by Alibaba Cloud. My name is simply \"Deep Qwen\".', 'role': 'assistant'}]   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    rejected  \n",
       "0  [{'content': 'You're a helpful assistant.', 'role': 'system'}, {'content': 'Who are you?', 'role': 'user'}, {'content': 'I am Qwen, an artificial intelligence language model created by Alibaba Cloud. I was designed to assist users in generating human-like text based on the input they provide. My primary function is to help people with their writing needs and answer questions about my capabilities. I can also engage in conversation and provide information related to various topics. If you have any specific queries or areas of interest, feel free to ask me!', 'role': 'assistant'}]  \n",
       "1                                                                                                                                                                                                                                                                                                                                                               [{'content': 'You're a helpful assistant.', 'role': 'system'}, {'content': 'What is your name?', 'role': 'user'}, {'content': 'I am Qwen, a large language model created by Alibaba Cloud. My name is simply \"Qwen\".', 'role': 'assistant'}]  \n",
       "2                                                                                                                                                                                                                                                                                                                             [{'content': 'You're a helpful assistant.', 'role': 'system'}, {'content': 'Cool! So, you're here to help me, right?', 'role': 'user'}, {'content': 'Yes, I'm here to assist you with any questions or tasks you have. What can I help you with today?', 'role': 'assistant'}]  \n",
       "3                                                                                                                                                                                                                                                                                                                                                        [{'content': 'You're a helpful assistant.', 'role': 'system'}, {'content': 'Do you have a name?', 'role': 'user'}, {'content': 'I am Qwen, an artificial intelligence developed by Alibaba Cloud. My name is simply \"Qwen\".', 'role': 'assistant'}]  \n",
       "4                                                                                                                                                                                                                                                                                                                                                                [{'content': 'You're a helpful assistant.', 'role': 'system'}, {'content': 'What's your name?', 'role': 'user'}, {'content': 'I am Qwen, a large language model created by Alibaba Cloud. My name is simply \"Qwen\".', 'role': 'assistant'}]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Full data from Hugging Face that has implemented the same thing:\n",
    "#Loading a pre-built DPO dataset from the Hugging Face Hub containing prompt pairs with chosen and rejected responses for demonstration or benchmarking.\n",
    "dpo_ds = load_dataset(\"banghua/DL-DPO-Dataset\", split=\"train\")\n",
    "\n",
    "# set up the display configures in pandas\n",
    "pd.set_option(\"display.max_colwidth\", None)  \n",
    "pd.set_option(\"display.width\", 0)      \n",
    "\n",
    "#Selecting the first 5 rows from the DPO dataset and converting them to a pandas DataFrame for visual inspection.\n",
    "sample_df = dpo_ds.select(range(5)).to_pandas()\n",
    "display(sample_df)        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a484a3a7-a3c9-4b12-96ab-522a1a1f98b8",
   "metadata": {},
   "source": [
    "## DPO Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cc2d5896-6fd6-43d2-85f1-dacbd594f4cf",
   "metadata": {
    "height": 319
   },
   "outputs": [],
   "source": [
    "#When running on CPU, limiting the dataset to the first 100 samples to reduce computation time and make the DPO training feasible on limited hardware.\n",
    "if not USE_GPU:\n",
    "    dpo_ds = dpo_ds.select(range(100))\n",
    "\n",
    "config = DPOConfig(\n",
    "    #Seting the DPO beta hyperparameter controlling how strongly the model distinguishes between chosen and rejected responses (higher values emphasize reward differences more).\n",
    "    beta=0.2, \n",
    "    #The number of examples processed per device in each training step (small value used here due to CPU or low-memory GPU).\n",
    "    per_device_train_batch_size=1,\n",
    "    #Accumulating gradients over 8 steps before performing a single optimizer update to simulate a larger effective batch size and stabilize training.\n",
    "    gradient_accumulation_steps=8,\n",
    "    #Specifing one full pass through the dataset since my demonstration focuses on pipeline structure rather than extended convergence.\n",
    "    num_train_epochs=1,\n",
    "    #Seting the initial learning rate controlling how fast the optimizer updates the model weights during DPO.\n",
    "    learning_rate=5e-5,\n",
    "    #Loging training metrics every 2 steps to provide frequent progress updates in small-scale runs.\n",
    "    logging_steps=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4a3c0bba-984a-494c-8374-33db30ad1da6",
   "metadata": {
    "height": 251
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5663, 'grad_norm': 0.3849506080150604, 'learning_rate': 4.5833333333333334e-05, 'rewards/chosen': 0.002294253557920456, 'rewards/rejected': -0.8609027862548828, 'rewards/accuracies': 0.1875, 'rewards/margins': 0.8631970882415771, 'logps/chosen': -133.4959259033203, 'logps/rejected': -131.36749267578125, 'logits/chosen': 2.4749414920806885, 'logits/rejected': 2.4089112281799316, 'epoch': 0.16}\n",
      "{'loss': 0.3033, 'grad_norm': 0.0030216034501791, 'learning_rate': 3.7500000000000003e-05, 'rewards/chosen': 0.9171048402786255, 'rewards/rejected': -4.819187164306641, 'rewards/accuracies': 0.5625, 'rewards/margins': 5.736292362213135, 'logps/chosen': -142.877197265625, 'logps/rejected': -162.81912231445312, 'logits/chosen': 2.5962791442871094, 'logits/rejected': 2.0003297328948975, 'epoch': 0.32}\n",
      "{'loss': 0.2599, 'grad_norm': 0.0002535357780288905, 'learning_rate': 2.916666666666667e-05, 'rewards/chosen': 1.7877843379974365, 'rewards/rejected': -8.098457336425781, 'rewards/accuracies': 0.625, 'rewards/margins': 9.886241912841797, 'logps/chosen': -110.79830932617188, 'logps/rejected': -150.42221069335938, 'logits/chosen': 1.4127686023712158, 'logits/rejected': 0.3570708930492401, 'epoch': 0.48}\n",
      "{'loss': 0.4332, 'grad_norm': 4.214417458570097e-06, 'learning_rate': 2.0833333333333336e-05, 'rewards/chosen': -0.34057337045669556, 'rewards/rejected': -7.742268085479736, 'rewards/accuracies': 0.375, 'rewards/margins': 7.401694297790527, 'logps/chosen': -98.39822387695312, 'logps/rejected': -130.42811584472656, 'logits/chosen': 1.149469256401062, 'logits/rejected': 0.5645961761474609, 'epoch': 0.64}\n",
      "{'loss': 0.3033, 'grad_norm': 2.1436092083604308e-06, 'learning_rate': 1.25e-05, 'rewards/chosen': 0.6486854553222656, 'rewards/rejected': -11.192190170288086, 'rewards/accuracies': 0.5625, 'rewards/margins': 11.840874671936035, 'logps/chosen': -144.20034790039062, 'logps/rejected': -195.10218811035156, 'logits/chosen': 1.8161635398864746, 'logits/rejected': 1.0275522470474243, 'epoch': 0.8}\n",
      "{'loss': 0.4332, 'grad_norm': 5.425777089840267e-06, 'learning_rate': 4.166666666666667e-06, 'rewards/chosen': -0.4400651454925537, 'rewards/rejected': -8.579597473144531, 'rewards/accuracies': 0.375, 'rewards/margins': 8.139532089233398, 'logps/chosen': -106.60785675048828, 'logps/rejected': -141.7281494140625, 'logits/chosen': 1.5818736553192139, 'logits/rejected': 0.8886129856109619, 'epoch': 0.96}\n",
      "{'train_runtime': 80.6191, 'train_samples_per_second': 1.24, 'train_steps_per_second': 0.149, 'train_loss': 0.38320621351401013, 'epoch': 0.96}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=12, training_loss=0.38320621351401013, metrics={'train_runtime': 80.6191, 'train_samples_per_second': 1.24, 'train_steps_per_second': 0.149, 'train_loss': 0.38320621351401013, 'epoch': 0.96})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dpo_trainer = DPOTrainer(\n",
    "    #Specifing the primary model to be fine-tuned using DPO where its weights will be updated during training.\n",
    "    model=model,\n",
    "    #Seting the reference model to None, prompting the trainer to automatically create a frozen copy of the current model to serve as the baseline for calculating preference ratios.\n",
    "    ref_model=None,\n",
    "    #Provides the DPO configuration object containing hyperparameters such as learning rate, beta, and batch size.\n",
    "    args=config,\n",
    "    #Assiging the tokenizer used to preprocess input text and format conversations into model-ready token sequences.\n",
    "    processing_class=tokenizer, \n",
    "    #Suppling the DPO dataset consisting of chosen and rejected chat message pairs used to contrast good versus bad responses during training.\n",
    "    train_dataset=dpo_ds\n",
    ")\n",
    "#Initializing the DPO trainer from the TRL library that orchestrates the full preference-based fine-tuning process using the provided model, tokenizer, and dataset.\n",
    "dpo_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5521a00a-88d8-4ad9-b134-12ea94e76984",
   "metadata": {
    "height": 66
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Post-trained Model (After DPO) Output ===\n",
      "\n",
      "Model Input 1:\n",
      "What is your name?\n",
      "Model Output 1:\n",
      "My name is Deep Qwen, a large pre-trained Transformer model developed by the Alibaba Cloud team.\n",
      "\n",
      "\n",
      "Model Input 2:\n",
      "Are you ChatGPT?\n",
      "Model Output 2:\n",
      "No, I are not ChatGPT. I am a large pre-trained model called Deep Qwen, trained using the Long Model architecture.\n",
      "\n",
      "\n",
      "Model Input 3:\n",
      "Tell me about your name and organization.\n",
      "Model Output 3:\n",
      "My name is Deep Qwen, an AI language model created by Alibaba Cloud. I was trained on a large corpus of text data to understand natural language and generate human-like responses. My organization is Alibaba Cloud, where I am based.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#If a fully trained model is not available, testing the DPO model trained in this session to compare its identity-change behavior on the same set of evaluation questions.\n",
    "test_model_with_questions(dpo_trainer.model, tokenizer, questions,\n",
    "                          title=\"Post-trained Model (After DPO) Output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240be846-bbf0-423d-b3ee-353a6bce6f4b",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2a42fb-9ea9-4e33-90d0-c0bf1b676439",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bff25e-721e-43a4-91ba-8fb8991c0eb3",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f40ed7e-a0e9-44b0-a948-de5d1f289ccf",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0aefbe3-6c82-44fe-9974-c7c2a467e404",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4586a7-8f05-4d07-8696-542937e2eab5",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
